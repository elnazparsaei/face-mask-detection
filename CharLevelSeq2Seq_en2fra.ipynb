{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elnazparsaei/face-mask-detection/blob/main/CharLevelSeq2Seq_en2fra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b14f9772-1863-4101-a5bb-8e29805e0e58",
      "metadata": {
        "id": "b14f9772-1863-4101-a5bb-8e29805e0e58"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from __future__ import print_function\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5c11732f-4638-4d0e-babd-ebc9caba6385",
      "metadata": {
        "id": "5c11732f-4638-4d0e-babd-ebc9caba6385"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "epochs = 300\n",
        "latent_dim = 256\n",
        "num_samples = 10000\n",
        "data_path = 'fra.txt'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL5ERMUsc0HK",
        "outputId": "ac7aff79-ee83-45be-a1a5-b6f301876b66"
      },
      "id": "wL5ERMUsc0HK",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2d8aa198-ed08-48a6-8015-71ca7b604d69",
      "metadata": {
        "id": "2d8aa198-ed08-48a6-8015-71ca7b604d69"
      },
      "outputs": [],
      "source": [
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "lines = open(data_path, encoding='utf8').read().split('\\n')\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split('\\t')\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Seq2SeqTranslation')\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Nyf9pxyIYUxN",
        "outputId": "0fa1a037-5784-4dc8-ec09-1d5e6f5e121d"
      },
      "id": "Nyf9pxyIYUxN",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Seq2SeqTranslation'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JydoYEotWalI",
        "outputId": "a81e1abc-9b1c-4219-891d-316667720830"
      },
      "id": "JydoYEotWalI",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "67ab98ef-1281-4c6f-9280-413b225e9ae4",
      "metadata": {
        "id": "67ab98ef-1281-4c6f-9280-413b225e9ae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "507dfbf8-1d73-4505-be5b-a4b8b4392276"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Run!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "input_texts[12]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "983f14b1-8fd7-4e71-a3f6-a9f8fd2e8590",
      "metadata": {
        "id": "983f14b1-8fd7-4e71-a3f6-a9f8fd2e8590",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e681269a-75dd-4331-a55c-96bfc65ffd70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\tFuyez !\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "target_texts[12]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1494bc8d-ffce-41bb-a6b3-d5184b292004",
      "metadata": {
        "id": "1494bc8d-ffce-41bb-a6b3-d5184b292004"
      },
      "outputs": [],
      "source": [
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4ec6113c-8fa3-4ea9-a137-32284baee8ba",
      "metadata": {
        "id": "4ec6113c-8fa3-4ea9-a137-32284baee8ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "3e5f5d68-0638-4a6b-e57b-c5ebdb42bf35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ',\n",
              " '!',\n",
              " '\"',\n",
              " '$',\n",
              " '%',\n",
              " '&',\n",
              " \"'\",\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '5',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'Y',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "input_characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c7fb28be-9252-4fe4-88b2-7cd6dbc5eb26",
      "metadata": {
        "id": "c7fb28be-9252-4fe4-88b2-7cd6dbc5eb26",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "de12d979-4cb1-45a5-b122-572ed00858f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\t',\n",
              " '\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '%',\n",
              " '&',\n",
              " \"'\",\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '5',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'Y',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '\\xa0',\n",
              " '«',\n",
              " '»',\n",
              " 'À',\n",
              " 'Ç',\n",
              " 'É',\n",
              " 'Ê',\n",
              " 'à',\n",
              " 'â',\n",
              " 'ç',\n",
              " 'è',\n",
              " 'é',\n",
              " 'ê',\n",
              " 'î',\n",
              " 'ï',\n",
              " 'ô',\n",
              " 'ù',\n",
              " 'û',\n",
              " 'œ',\n",
              " '\\u2009',\n",
              " '’',\n",
              " '\\u202f']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "target_characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a033e62d-72b2-4c36-86ee-e8952d1da109",
      "metadata": {
        "id": "a033e62d-72b2-4c36-86ee-e8952d1da109",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fec33b0-a881-4d91-f162-7b2398d4804d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 10000\n",
            "Number of unique input tokens: 70\n",
            "Number of unique output tokens: 91\n",
            "Max sequence length for inputs: 14\n",
            "Max sequence length for outputs: 59\n"
          ]
        }
      ],
      "source": [
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "52474aa2-b4ff-4396-b740-5908996c7b81",
      "metadata": {
        "id": "52474aa2-b4ff-4396-b740-5908996c7b81"
      },
      "outputs": [],
      "source": [
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "#sakhte tensorhaee ke hame anasor dakhelishun sefre\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c2cb417c-e53a-4d40-a65d-71f129fe7b81",
      "metadata": {
        "id": "c2cb417c-e53a-4d40-a65d-71f129fe7b81"
      },
      "outputs": [],
      "source": [
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "bf4a6897-b920-4125-8a41-abb09bd8d38b",
      "metadata": {
        "id": "bf4a6897-b920-4125-8a41-abb09bd8d38b"
      },
      "outputs": [],
      "source": [
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)# latent_dim=tedade units\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "cb3fc4cb-3bef-414e-bc18-03d05dafb972",
      "metadata": {
        "id": "cb3fc4cb-3bef-414e-bc18-03d05dafb972"
      },
      "outputs": [],
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4bb4328b-8c40-4315-9c8f-09c1ab8b214b",
      "metadata": {
        "id": "4bb4328b-8c40-4315-9c8f-09c1ab8b214b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06c99a04-98e6-4bb9-bf77-006d13b759f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, None, 70)]           0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, None, 91)]           0         []                            \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 [(None, 256),                334848    ['input_1[0][0]']             \n",
            "                              (None, 256),                                                        \n",
            "                              (None, 256)]                                                        \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, None, 256),          356352    ['input_2[0][0]',             \n",
            "                              (None, 256),                           'lstm[0][1]',                \n",
            "                              (None, 256)]                           'lstm[0][2]']                \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, None, 91)             23387     ['lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 714587 (2.73 MB)\n",
            "Trainable params: 714587 (2.73 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "fd918890-b78b-42f5-a980-0f015db6b8e7",
      "metadata": {
        "id": "fd918890-b78b-42f5-a980-0f015db6b8e7"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a61ab693-5801-4455-9111-62bc9c211b08",
      "metadata": {
        "id": "a61ab693-5801-4455-9111-62bc9c211b08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b40c15e-2b58-4bbe-9938-d9686fd523f4",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "125/125 [==============================] - 7s 29ms/step - loss: 1.0509 - val_loss: 1.1166\n",
            "Epoch 2/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.9826 - val_loss: 1.0768\n",
            "Epoch 3/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.9657 - val_loss: 1.0806\n",
            "Epoch 4/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.9529 - val_loss: 1.0696\n",
            "Epoch 5/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.9426 - val_loss: 1.0740\n",
            "Epoch 6/300\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.9338 - val_loss: 1.0317\n",
            "Epoch 7/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.9251 - val_loss: 1.0293\n",
            "Epoch 8/300\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.9160 - val_loss: 1.0043\n",
            "Epoch 9/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.9069 - val_loss: 1.0010\n",
            "Epoch 10/300\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.8977 - val_loss: 0.9934\n",
            "Epoch 11/300\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.8884 - val_loss: 0.9843\n",
            "Epoch 12/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.8793 - val_loss: 0.9703\n",
            "Epoch 13/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.8718 - val_loss: 0.9690\n",
            "Epoch 14/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.8627 - val_loss: 0.9608\n",
            "Epoch 15/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.8557 - val_loss: 0.9668\n",
            "Epoch 16/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.8497 - val_loss: 0.9410\n",
            "Epoch 17/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.8413 - val_loss: 0.9184\n",
            "Epoch 18/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.8352 - val_loss: 0.9188\n",
            "Epoch 19/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.8280 - val_loss: 0.9424\n",
            "Epoch 20/300\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.8222 - val_loss: 0.9325\n",
            "Epoch 21/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.8179 - val_loss: 0.8982\n",
            "Epoch 22/300\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.8106 - val_loss: 0.8787\n",
            "Epoch 23/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.8065 - val_loss: 0.8812\n",
            "Epoch 24/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.8017 - val_loss: 0.8868\n",
            "Epoch 25/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7961 - val_loss: 0.8597\n",
            "Epoch 26/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7922 - val_loss: 0.8629\n",
            "Epoch 27/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7877 - val_loss: 0.8770\n",
            "Epoch 28/300\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.7843 - val_loss: 0.8698\n",
            "Epoch 29/300\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.7810 - val_loss: 0.8529\n",
            "Epoch 30/300\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.7766 - val_loss: 0.9004\n",
            "Epoch 31/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.7749 - val_loss: 0.8573\n",
            "Epoch 32/300\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.7708 - val_loss: 0.8553\n",
            "Epoch 33/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7684 - val_loss: 0.8689\n",
            "Epoch 34/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7649 - val_loss: 0.8454\n",
            "Epoch 35/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7615 - val_loss: 0.8625\n",
            "Epoch 36/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7591 - val_loss: 0.8527\n",
            "Epoch 37/300\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.7561 - val_loss: 0.8418\n",
            "Epoch 38/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7534 - val_loss: 0.8390\n",
            "Epoch 39/300\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.7521 - val_loss: 0.8441\n",
            "Epoch 40/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.7493 - val_loss: 0.8321\n",
            "Epoch 41/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.7470 - val_loss: 0.8251\n",
            "Epoch 42/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7435 - val_loss: 0.8207\n",
            "Epoch 43/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7434 - val_loss: 0.8328\n",
            "Epoch 44/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7378 - val_loss: 0.8126\n",
            "Epoch 45/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7376 - val_loss: 0.8296\n",
            "Epoch 46/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7339 - val_loss: 0.8243\n",
            "Epoch 47/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7314 - val_loss: 0.8432\n",
            "Epoch 48/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7300 - val_loss: 0.8260\n",
            "Epoch 49/300\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.7269 - val_loss: 0.8182\n",
            "Epoch 50/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.7263 - val_loss: 0.8291\n",
            "Epoch 51/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.7247 - val_loss: 0.8258\n",
            "Epoch 52/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7225 - val_loss: 0.8094\n",
            "Epoch 53/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7203 - val_loss: 0.8083\n",
            "Epoch 54/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7208 - val_loss: 0.8085\n",
            "Epoch 55/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7172 - val_loss: 0.8062\n",
            "Epoch 56/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7149 - val_loss: 0.8031\n",
            "Epoch 57/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7125 - val_loss: 0.7937\n",
            "Epoch 58/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7110 - val_loss: 0.8079\n",
            "Epoch 59/300\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.7102 - val_loss: 0.7908\n",
            "Epoch 60/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.7091 - val_loss: 0.7948\n",
            "Epoch 61/300\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.7074 - val_loss: 0.7988\n",
            "Epoch 62/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7054 - val_loss: 0.8207\n",
            "Epoch 63/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7047 - val_loss: 0.7827\n",
            "Epoch 64/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7026 - val_loss: 0.8198\n",
            "Epoch 65/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7001 - val_loss: 0.8450\n",
            "Epoch 66/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7005 - val_loss: 0.7989\n",
            "Epoch 67/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6973 - val_loss: 0.7792\n",
            "Epoch 68/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6975 - val_loss: 0.7860\n",
            "Epoch 69/300\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.6942 - val_loss: 0.8016\n",
            "Epoch 70/300\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.6939 - val_loss: 0.7912\n",
            "Epoch 71/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.6906 - val_loss: 0.7962\n",
            "Epoch 72/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6905 - val_loss: 0.7829\n",
            "Epoch 73/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6916 - val_loss: 0.7910\n",
            "Epoch 74/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6883 - val_loss: 0.7946\n",
            "Epoch 75/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6878 - val_loss: 0.7872\n",
            "Epoch 76/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6855 - val_loss: 0.7679\n",
            "Epoch 77/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6859 - val_loss: 0.7680\n",
            "Epoch 78/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6824 - val_loss: 0.7910\n",
            "Epoch 79/300\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.6819 - val_loss: 0.7852\n",
            "Epoch 80/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.6820 - val_loss: 0.7716\n",
            "Epoch 81/300\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.6816 - val_loss: 0.7794\n",
            "Epoch 82/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6784 - val_loss: 0.7871\n",
            "Epoch 83/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6772 - val_loss: 0.7826\n",
            "Epoch 84/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6759 - val_loss: 0.7726\n",
            "Epoch 85/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6739 - val_loss: 0.7854\n",
            "Epoch 86/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6727 - val_loss: 0.7980\n",
            "Epoch 87/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6717 - val_loss: 0.7620\n",
            "Epoch 88/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6716 - val_loss: 0.7691\n",
            "Epoch 89/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.6701 - val_loss: 0.7777\n",
            "Epoch 90/300\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.6701 - val_loss: 0.7752\n",
            "Epoch 91/300\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.6675 - val_loss: 0.7752\n",
            "Epoch 92/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6663 - val_loss: 0.7696\n",
            "Epoch 93/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6657 - val_loss: 0.7821\n",
            "Epoch 94/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6633 - val_loss: 0.7566\n",
            "Epoch 95/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6648 - val_loss: 0.7709\n",
            "Epoch 96/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6635 - val_loss: 0.7742\n",
            "Epoch 97/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6600 - val_loss: 0.7565\n",
            "Epoch 98/300\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.6596 - val_loss: 0.7585\n",
            "Epoch 99/300\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.6616 - val_loss: 0.7811\n",
            "Epoch 100/300\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.6578 - val_loss: 0.7615\n",
            "Epoch 101/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6565 - val_loss: 0.7624\n",
            "Epoch 102/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6558 - val_loss: 0.7778\n",
            "Epoch 103/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6539 - val_loss: 0.7586\n",
            "Epoch 104/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6561 - val_loss: 0.7672\n",
            "Epoch 105/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6558 - val_loss: 0.7767\n",
            "Epoch 106/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6532 - val_loss: 0.7642\n",
            "Epoch 107/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6522 - val_loss: 0.7567\n",
            "Epoch 108/300\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.6505 - val_loss: 0.7652\n",
            "Epoch 109/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.6536 - val_loss: 0.7607\n",
            "Epoch 110/300\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.6503 - val_loss: 0.7542\n",
            "Epoch 111/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6495 - val_loss: 0.7510\n",
            "Epoch 112/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6502 - val_loss: 0.7588\n",
            "Epoch 113/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6459 - val_loss: 0.7531\n",
            "Epoch 114/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6461 - val_loss: 0.7581\n",
            "Epoch 115/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6457 - val_loss: 0.7603\n",
            "Epoch 116/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6447 - val_loss: 0.7502\n",
            "Epoch 117/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6420 - val_loss: 0.7514\n",
            "Epoch 118/300\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.6423 - val_loss: 0.7517\n",
            "Epoch 119/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.6390 - val_loss: 0.7439\n",
            "Epoch 120/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.6437 - val_loss: 0.7438\n",
            "Epoch 121/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6417 - val_loss: 0.7931\n",
            "Epoch 122/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6388 - val_loss: 0.7538\n",
            "Epoch 123/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6389 - val_loss: 0.7564\n",
            "Epoch 124/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6374 - val_loss: 0.7500\n",
            "Epoch 125/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6423 - val_loss: 0.7687\n",
            "Epoch 126/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6351 - val_loss: 0.7391\n",
            "Epoch 127/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6368 - val_loss: 0.7548\n",
            "Epoch 128/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.6329 - val_loss: 0.7493\n",
            "Epoch 129/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.6332 - val_loss: 0.7418\n",
            "Epoch 130/300\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.6317 - val_loss: 0.7555\n",
            "Epoch 131/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6313 - val_loss: 0.7466\n",
            "Epoch 132/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6307 - val_loss: 0.7372\n",
            "Epoch 133/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6357 - val_loss: 0.7489\n",
            "Epoch 134/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6328 - val_loss: 0.7440\n",
            "Epoch 135/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6315 - val_loss: 0.7645\n",
            "Epoch 136/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6391 - val_loss: 0.7399\n",
            "Epoch 137/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6285 - val_loss: 0.7594\n",
            "Epoch 138/300\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.6267 - val_loss: 0.7591\n",
            "Epoch 139/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.6255 - val_loss: 0.7428\n",
            "Epoch 140/300\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.6244 - val_loss: 0.7324\n",
            "Epoch 141/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6306 - val_loss: 0.7493\n",
            "Epoch 142/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6220 - val_loss: 0.7679\n",
            "Epoch 143/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6249 - val_loss: 0.7368\n",
            "Epoch 144/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6224 - val_loss: 0.7450\n",
            "Epoch 145/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6229 - val_loss: 0.7411\n",
            "Epoch 146/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6188 - val_loss: 0.7602\n",
            "Epoch 147/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6213 - val_loss: 0.7370\n",
            "Epoch 148/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.6200 - val_loss: 0.7414\n",
            "Epoch 149/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.6170 - val_loss: 0.7304\n",
            "Epoch 150/300\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.6195 - val_loss: 0.7251\n",
            "Epoch 151/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6163 - val_loss: 0.7496\n",
            "Epoch 152/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6163 - val_loss: 0.7323\n",
            "Epoch 153/300\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.6150 - val_loss: 0.7364\n",
            "Epoch 154/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.6203 - val_loss: 0.7327\n",
            "Epoch 155/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.6151 - val_loss: 0.7302\n",
            "Epoch 156/300\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.6139 - val_loss: 0.7401\n",
            "Epoch 157/300\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.6135 - val_loss: 0.7273\n",
            "Epoch 158/300\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.6105 - val_loss: 0.7272\n",
            "Epoch 159/300\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.6117 - val_loss: 0.7377\n",
            "Epoch 160/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6124 - val_loss: 0.7361\n",
            "Epoch 161/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6097 - val_loss: 0.7270\n",
            "Epoch 162/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6099 - val_loss: 0.7315\n",
            "Epoch 163/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6075 - val_loss: 0.7197\n",
            "Epoch 164/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6069 - val_loss: 0.7409\n",
            "Epoch 165/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6081 - val_loss: 0.7265\n",
            "Epoch 166/300\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.6059 - val_loss: 0.7287\n",
            "Epoch 167/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.6054 - val_loss: 0.7378\n",
            "Epoch 168/300\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.6073 - val_loss: 0.7267\n",
            "Epoch 169/300\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.6034 - val_loss: 0.7475\n",
            "Epoch 170/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6022 - val_loss: 0.7343\n",
            "Epoch 171/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6013 - val_loss: 0.7308\n",
            "Epoch 172/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6018 - val_loss: 0.7297\n",
            "Epoch 173/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6032 - val_loss: 0.7250\n",
            "Epoch 174/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5995 - val_loss: 0.7345\n",
            "Epoch 175/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6017 - val_loss: 0.7316\n",
            "Epoch 176/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.6036 - val_loss: 0.7364\n",
            "Epoch 177/300\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.5992 - val_loss: 0.7130\n",
            "Epoch 178/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.6024 - val_loss: 0.7431\n",
            "Epoch 179/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5966 - val_loss: 0.7250\n",
            "Epoch 180/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5981 - val_loss: 0.7326\n",
            "Epoch 181/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5980 - val_loss: 0.7253\n",
            "Epoch 182/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5985 - val_loss: 0.7178\n",
            "Epoch 183/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5979 - val_loss: 0.7190\n",
            "Epoch 184/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5963 - val_loss: 0.7326\n",
            "Epoch 185/300\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.5965 - val_loss: 0.7317\n",
            "Epoch 186/300\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.5958 - val_loss: 0.7196\n",
            "Epoch 187/300\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.5968 - val_loss: 0.7204\n",
            "Epoch 188/300\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.5937 - val_loss: 0.7186\n",
            "Epoch 189/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5977 - val_loss: 0.7308\n",
            "Epoch 190/300\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.5952 - val_loss: 0.7393\n",
            "Epoch 191/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5953 - val_loss: 0.7109\n",
            "Epoch 192/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5947 - val_loss: 0.7137\n",
            "Epoch 193/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5938 - val_loss: 0.7147\n",
            "Epoch 194/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5912 - val_loss: 0.7224\n",
            "Epoch 195/300\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.5944 - val_loss: 0.7174\n",
            "Epoch 196/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.5902 - val_loss: 0.7221\n",
            "Epoch 197/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.5920 - val_loss: 0.7178\n",
            "Epoch 198/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5931 - val_loss: 0.7209\n",
            "Epoch 199/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5869 - val_loss: 0.7116\n",
            "Epoch 200/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5901 - val_loss: 0.7197\n",
            "Epoch 201/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5901 - val_loss: 0.7188\n",
            "Epoch 202/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5855 - val_loss: 0.7130\n",
            "Epoch 203/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5884 - val_loss: 0.7214\n",
            "Epoch 204/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5868 - val_loss: 0.7196\n",
            "Epoch 205/300\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.5885 - val_loss: 0.7255\n",
            "Epoch 206/300\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.5885 - val_loss: 0.7347\n",
            "Epoch 207/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.5890 - val_loss: 0.7239\n",
            "Epoch 208/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5866 - val_loss: 0.7157\n",
            "Epoch 209/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5831 - val_loss: 0.7189\n",
            "Epoch 210/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5865 - val_loss: 0.7173\n",
            "Epoch 211/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5839 - val_loss: 0.7238\n",
            "Epoch 212/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5850 - val_loss: 0.7173\n",
            "Epoch 213/300\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.5807 - val_loss: 0.7260\n",
            "Epoch 214/300\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.5843 - val_loss: 0.7328\n",
            "Epoch 215/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.5835 - val_loss: 0.7116\n",
            "Epoch 216/300\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.5840 - val_loss: 0.7130\n",
            "Epoch 217/300\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.5793 - val_loss: 0.7049\n",
            "Epoch 218/300\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.5778 - val_loss: 0.7236\n",
            "Epoch 219/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5783 - val_loss: 0.7134\n",
            "Epoch 220/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5868 - val_loss: 0.7215\n",
            "Epoch 221/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5830 - val_loss: 0.7166\n",
            "Epoch 222/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5792 - val_loss: 0.7071\n",
            "Epoch 223/300\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.5781 - val_loss: 0.7156\n",
            "Epoch 224/300\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.5755 - val_loss: 0.7149\n",
            "Epoch 225/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.5832 - val_loss: 0.7268\n",
            "Epoch 226/300\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.5791 - val_loss: 0.7150\n",
            "Epoch 227/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5772 - val_loss: 0.7193\n",
            "Epoch 228/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5780 - val_loss: 0.7159\n",
            "Epoch 229/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5768 - val_loss: 0.7113\n",
            "Epoch 230/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5706 - val_loss: 0.7028\n",
            "Epoch 231/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5794 - val_loss: 0.7114\n",
            "Epoch 232/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5738 - val_loss: 0.7094\n",
            "Epoch 233/300\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.5713 - val_loss: 0.7157\n",
            "Epoch 234/300\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.5697 - val_loss: 0.7047\n",
            "Epoch 235/300\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.5723 - val_loss: 0.7062\n",
            "Epoch 236/300\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.5717 - val_loss: 0.7001\n",
            "Epoch 237/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5756 - val_loss: 0.7315\n",
            "Epoch 238/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5729 - val_loss: 0.7260\n",
            "Epoch 239/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5725 - val_loss: 0.7439\n",
            "Epoch 240/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5726 - val_loss: 0.7040\n",
            "Epoch 241/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5737 - val_loss: 0.7061\n",
            "Epoch 242/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5707 - val_loss: 0.7149\n",
            "Epoch 243/300\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.5732 - val_loss: 0.7467\n",
            "Epoch 244/300\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.5745 - val_loss: 0.7167\n",
            "Epoch 245/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.5712 - val_loss: 0.7047\n",
            "Epoch 246/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5694 - val_loss: 0.7123\n",
            "Epoch 247/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5699 - val_loss: 0.7023\n",
            "Epoch 248/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5711 - val_loss: 0.7166\n",
            "Epoch 249/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5685 - val_loss: 0.7046\n",
            "Epoch 250/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5640 - val_loss: 0.7001\n",
            "Epoch 251/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5660 - val_loss: 0.7045\n",
            "Epoch 252/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5637 - val_loss: 0.7110\n",
            "Epoch 253/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.5616 - val_loss: 0.7048\n",
            "Epoch 254/300\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.5664 - val_loss: 0.7051\n",
            "Epoch 255/300\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.5636 - val_loss: 0.7035\n",
            "Epoch 256/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5622 - val_loss: 0.7080\n",
            "Epoch 257/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5618 - val_loss: 0.7312\n",
            "Epoch 258/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5655 - val_loss: 0.7009\n",
            "Epoch 259/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5685 - val_loss: 0.7195\n",
            "Epoch 260/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5661 - val_loss: 0.7188\n",
            "Epoch 261/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5628 - val_loss: 0.7016\n",
            "Epoch 262/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5623 - val_loss: 0.7072\n",
            "Epoch 263/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.5621 - val_loss: 0.7031\n",
            "Epoch 264/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.5609 - val_loss: 0.7088\n",
            "Epoch 265/300\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.5589 - val_loss: 0.7072\n",
            "Epoch 266/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5648 - val_loss: 0.7061\n",
            "Epoch 267/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5657 - val_loss: 0.7316\n",
            "Epoch 268/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5673 - val_loss: 0.7365\n",
            "Epoch 269/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5681 - val_loss: 0.7150\n",
            "Epoch 270/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5652 - val_loss: 0.7082\n",
            "Epoch 271/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5649 - val_loss: 0.7219\n",
            "Epoch 272/300\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.5612 - val_loss: 0.7033\n",
            "Epoch 273/300\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.5587 - val_loss: 0.7086\n",
            "Epoch 274/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.5617 - val_loss: 0.7011\n",
            "Epoch 275/300\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.5592 - val_loss: 0.7303\n",
            "Epoch 276/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5588 - val_loss: 0.7032\n",
            "Epoch 277/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5570 - val_loss: 0.7045\n",
            "Epoch 278/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5527 - val_loss: 0.7227\n",
            "Epoch 279/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5540 - val_loss: 0.7039\n",
            "Epoch 280/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5548 - val_loss: 0.7022\n",
            "Epoch 281/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5543 - val_loss: 0.7068\n",
            "Epoch 282/300\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.5583 - val_loss: 0.7029\n",
            "Epoch 283/300\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.5568 - val_loss: 0.7011\n",
            "Epoch 284/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.5553 - val_loss: 0.6993\n",
            "Epoch 285/300\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.5567 - val_loss: 0.7066\n",
            "Epoch 286/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5564 - val_loss: 0.7011\n",
            "Epoch 287/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5579 - val_loss: 0.7037\n",
            "Epoch 288/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5580 - val_loss: 0.7199\n",
            "Epoch 289/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5538 - val_loss: 0.7028\n",
            "Epoch 290/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5553 - val_loss: 0.6958\n",
            "Epoch 291/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5540 - val_loss: 0.7031\n",
            "Epoch 292/300\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.5552 - val_loss: 0.7077\n",
            "Epoch 293/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.5541 - val_loss: 0.7055\n",
            "Epoch 294/300\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.5515 - val_loss: 0.7032\n",
            "Epoch 295/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.5516 - val_loss: 0.6957\n",
            "Epoch 296/300\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.5485 - val_loss: 0.6990\n",
            "Epoch 297/300\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.5558 - val_loss: 0.7035\n",
            "Epoch 298/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5483 - val_loss: 0.7031\n",
            "Epoch 299/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5480 - val_loss: 0.6960\n",
            "Epoch 300/300\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5486 - val_loss: 0.7001\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fb8760b60b0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "ee1852d1-4221-4d70-9c9f-79ca4d498c4c",
      "metadata": {
        "id": "ee1852d1-4221-4d70-9c9f-79ca4d498c4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "724e6fa0-256c-436e-b1d5-47c20ad08a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save('e2f.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "4d011b4e-afc7-488b-87cc-b4a0bd6bf906",
      "metadata": {
        "id": "4d011b4e-afc7-488b-87cc-b4a0bd6bf906"
      },
      "outputs": [],
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "2962c3f2-21e8-4c2a-8b43-f3dd35116498",
      "metadata": {
        "id": "2962c3f2-21e8-4c2a-8b43-f3dd35116498"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "516b871c-0b62-4038-8376-0eb90718cce6",
      "metadata": {
        "collapsed": true,
        "id": "516b871c-0b62-4038-8376-0eb90718cce6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4729a854-c546-49d5-b5b6-b41cc99eb7a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "encoder_input_data[0: 0 + 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "b86c55e6-95eb-4289-8e4f-3e117b7b312c",
      "metadata": {
        "id": "b86c55e6-95eb-4289-8e4f-3e117b7b312c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48179168-7f87-4e18-a270-45232f93f24a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 680ms/step\n",
            "1/1 [==============================] - 1s 725ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Couseeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Couseeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Couseeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n"
          ]
        }
      ],
      "source": [
        "for seq_index in range(3):\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}